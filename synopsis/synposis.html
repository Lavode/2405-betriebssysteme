<h1>01</h1>

<h2>Def Betriebssystem:</h2>

<ul>
<li>Programme + Eigenschaften einer Rechenanlage, insbesondere Abwicklung und Steuerung von Programmen.</li>
<li>"Like a government", provide an environment within other programs can do useful work.</li>
<li>Bindeglied Software - Hardware</li>
</ul>

<h2>Speicherhierarchie</h2>

<p>Register - Cache - Hauptspeicher - SSD - Magnetische Disk - Optische Disk - Magnetbänder</p>

<h2>Architektur</h2>

<h3>Einprozessorsysteme</h3>

<ul>
<li>CPU &lt;- Daten, Interrupt, IO -> Gerät</li>
<li>CPU &lt;- Instruktionen, Daten -> Speicher</li>
<li>Gerät &lt;- DMA -> Speicher</li>
</ul>

<h3>Mehrprozessorsysteme</h3>

<p>Symmetrisch: Jeder Prozessor eigene Kopie von OS
Assymetrisch: Zuoordnung von Tasks auf Prozessoren (Scheduling)</p>

<h3>Clustersysteme</h3>

<p>Sammlung von Rechnern, gemeinsamer Speicher + Verbindung, gegenseitige Überwachung</p>

<h3>Sonstiges</h3>

<p>Verteilte Systeme: Verteilung auf heterogene Rechnersysteme, lose Kupplung, Client/Server
Cloud Computing: IaaS, public/private cloud, on-demand</p>

<h2>Betriebssystemtypen</h2>

<p>Stapelverarbeitung: Befehle auf Band, keine Interaktion
Serversysteme: Service-sharing (Drucker, ...), Multi-user
Multiprozessor: Anpassungen von Serversysteme für Multiprozessorsetup
PC-Betriebssysteme: Häufig nur ein Nutzer, einfache Oberfläche, viele IO / HID Geräte
Handheld Systeme: Teils Realtime, ereignisgesteuert
Embedded Systeme: Klein, effizient
Sensorsysteme: Energieeffizient, ereignisgesteurt
Echtzeitsysteme: Messgeräte, Roboter, Zeitschranken
Smart-Card Systeme: ...</p>

<h1>02</h1>

<h2>Systemkonponenten</h2>

<p>Prozessverwaltung, Hauptspeicherverwaltung, IO Systeme, Kommunikationssysteme, Sekundärspeicherverwaltung, Schutz und Sicherheit, Resourcenverwaltung allgemein(CPU, Memory, Disk)</p>

<h2>Dual-Mode</h2>

<p>User-mode für Anwendungen, System-mode für privilegierte (IO disk, memory, ...) Instruktionen.
Wechsel User -> System via trap oder interrupt
Wechsel System -> User durch System (reset Modus-Bit)</p>

<h2>Systembibliotheken</h2>

<p>Bilden Interface zwischen Prozessen und OS, stellen Funktionen bereit die zB System-Calls enthalten.</p>

<h2>Systemprogramme</h2>

<p>Umgebung für Entwicklung und Ausführung, Arbeit, ... (ls, cp, gcc, ...)</p>

<h2>Betriebssystemarchitektur</h2>

<p>Monolithisch (Linux, Solaris, Windows): Grupperiung aber keine Modularisierung, jeder kann jeden aufrufen
Geschichtet (OS X): A la TCP
Mikrokerne: Auslagerung Teils des OS in Server (evtl User-Space) -> Kleiner Kernel
Ereignisgesteuert: Klar
VM: Klar</p>

<h1>03</h1>

<h2>Prozess</h2>

<p>Code, Stack, Heap
Neu / Rechnend / Blockiert / Bereit / Beendet</p>

<h3>Prozessleitblock</h3>

<p>Zeiger / Zustand / Nummer / BfZähler / Register / ...</p>

<p>Verkettete PLBs bilden Warteschlange (doubly-linked-list)</p>

<h2>Thread</h2>

<p>Leichter Prozess, eigener Stack, teilt a) Code b) globale Daten c) OS Resourcen
Task == Ansammlung von Threads. (Ein Prozess = Task + 1 Thread)</p>

<p>Zustände wie bei Prozessor.</p>

<p>Thread kann arbeiten während andere Threads der Task blockiert. Effiziente Kontextwechsel (100x schneller als bei Prozess)
Aber: Scheduling auf Prozess-Stufe, d.h. Threads unsichtbar gegenüber OS - kein Schutz vor Starvation.</p>

<p>Bsp Browser mit Fenstern / Download Threads etc. Editor mit 1 Thread Formatierung, 1 Eingabe, ...</p>

<h3>User Threads</h3>

<p>Thread library, gleicher Adressraum, transparent gegen OS, müssen CPU freiwillig aufgeben</p>

<h3>Kernel Threads</h3>

<p>Von OS gemanaged, scheduling innerhalb Adressraum (z.B. bei blockierendem Thread)</p>

<h3>Hybride Threads</h3>

<p>Multiplexing von user threads auf kernel threads. Normalerweise N:M</p>

<h3>Thread Pools</h3>

<p>Spawnen von worker-Threads bei Start zwecks Vermeidung Overhead</p>

<h2>Scheduling</h2>

<p>Job Queue: Auf Massenspeicher abgelegte Prozesse (waiting to load)
Ready Queue: Prozesse bereit im Hauptspeicher
IO Queue: Prozesse die auf IO warten
Ausgelagerte Prozesse: Swapped</p>

<p>Scheduler: Wählt Prozess. </p>

<p>Dispatcher: Gibt Kontrolle an Prozess. (Speichert / lädt basierend auf PLB). Overhead 1-1000 micro-s</p>

<h1>04</h1>

<p>Prozessausführung: CPU Burts + IO Bursts</p>

<h2>Scheduler</h2>

<p>Wählt aus Ready-Queue jenen, der laufen soll.</p>

<p>Entscheid bei Wechsel:
- Running -> Blocked (non-preemptive)
- Running -> Ready (preemptive)
- Blocked -> Ready (preemptive)
- Process terminates (non-preemptive)</p>

<p>Kriterien: Fairness, CPU Last, Durchsatz, Wartezeit (Ready-Queue), Verweilzeit (Lebenszeit), Realzeitverhalten, ...</p>

<h3>Schätzung CPU Burst</h3>

<p>z.B. exponentieller Mittelwert: <code>t_n+1 = a * t+n + (1 - a) * t+n</code></p>

<h3>FCFS</h3>

<p>Nicht präemptiv.
Trivial</p>

<h3>Shortest Job First</h3>

<p>Nicht präemptiv: Keine Verdrängung
Präemptiv: Verdrängung</p>

<h3>Priorität</h3>

<p>Priorität basierend auf Speicherbedarf, EA, Wichtigkeit, ...
Problem: Aushungern. Lösung: Aging, Priorität steigt mit Wartezeit
Präemptiv oder nicht.</p>

<h3>Round Robin</h3>

<p>Gut für Time-Sharing
Jeder Prozess erhält Zeitquantum. Nach Ablauf, Einreihung in Ready-Queue.
Präemptiv</p>

<h3>Multilevel</h3>

<p>Mehrere Queues, jede Queue eigenes Scheduling, Aufteilung zwischen Queues mit Zeitscheiben</p>

<h3>Multilevel Feedback</h3>

<p>Mehrere Queues mit verschiedenen Zeitquanti. Präemptiv</p>

<h3>Lotterie</h3>

<p>Verlosung von Zeitquanti, Lose können zB von Client and Server gegeben werden</p>

<h3>Garantiertes Scheduling</h3>

<p>vorgesehene zeit = (aktuelle zeit - Erzeugungszeitpunkt) / Anzahl Prozesse
verbrauchte Zeit / vorgesehene Zeit = x
Wähle Prozess mit kleinstem X
Präemptiv</p>

<h3>Echtzeit</h3>

<p>Planbar falls:
Summe <code>(cpu_zeit_i / dauer_i) kleiner gleich 1</code></p>

<h4>Offline Scheduling</h4>

<p>Scheduling vor Start. Voraussetzung: Periodische Aktivitäten</p>

<h4>Earliest Deadline First</h4>

<p>Prozess mit engster Frist selektiert
Präemptiv / Nicht präemptiv</p>

<h1>05</h1>

<h2>Prozessinteraktion</h2>

<ul>
<li>Speicherbasiert (shared memory) -> synchronisation notwendig</li>
<li>Nachrichtenbasiert (sync / async, ...): Via Mailbox oder direkt
<ul>
<li>Synchron: 0 Kapazität: Erfordert direkte Synchronisation</li>
<li>Async endlich: Sender wartet falls voll, Empfänger falls leer</li>
<li>Async unendlich: Keine Wartezeiten</li>
</ul></li>
</ul>

<h2>Kritischer Abschnitt</h2>

<p>Folge von Code-Anweisungen mit Zugriff auf gemeinsame Daten.
Anforderung: Wechselseitiger Ausschluss, Fortschritt, Begrenztes Warten</p>

<h3>Semaphore</h3>

<p>Zählende: Initialisiert auf Anzahl Resourcen. wait() zählt runter, signal() hoch.</p>

<h3>Monitor</h3>

<p>Sammlung von Prozeduren, Variablen, Datenstrukturen. Innerhalb Monitor zu jedem Zeitpunkt nur ein aktiver Prozess.</p>

<h1>06</h1>

<h2>Verklemmungen</h2>

<p>A belegt Mittel das von B benötigt wird, B belegt Mittel das von A benötigt wird</p>

<p>Treten auf, wenn alle auftreten:
- Wechselseitiger Ausschluss (Resource nur von einem Prozess nutzbar)
- Halten und Warten: Prozess der Resource hält wartet auf andere
- Keine Verdrängung: Resource nur frewillig freigebbar
- Zirkluierendes Warten: Pi wartet auf Pi+1 mod n</p>

<h3>Verhindern</h3>

<p>Verhindern dass eine der vier Bedingungen zutrifft</p>

<p>Wechselseitiger Ausschluss: Nicht nötig für teilbare Resourcen
Halten und Warten: Belegen aller Resourcen vor Ausführung / Abgabe aller Mittel bevor neue belegt
Keine Verdrängung: Entzung von zugeweisenen Resourcen
Zirkulierndes Warten: Anordnung</p>

<h3>Vermeiden</h3>

<p>Für jeden Zugriff entscheiden ob dadurch Verklemmung auftreten könnte</p>

<ul>
<li>Jeder Prozess beschreibt maximal verwendete Resourcen</li>
<li>Anfrage erfüllt wenn in sicheren Zustand bleibt (keine Verklemmunge möglich)</li>
</ul>

<h4>Bankers</h4>

<p>Geeignet für Resourcen mit mehrfachen Instanzen
- Available[j] = k von j verfügbar
- Max[i, j] = Pi nutzt maximal k von j
- Allocation[i, j] = Pi nutzt aktuell k von j
- Need[i, j] = Max[i, j] - Allocation[i, j] = Pi nutzt maximal k von j zustätzlich</p>

<h3>Aufheben</h3>

<p>Erlauben dass auftritt, dann Massnahme ergreiffen</p>

<h1>07</h1>

<h2>Dynamisches Laden</h2>

<ul>
<li>Aufgerufene Routine checkt ob aufgerufene Routine geladen. Wenn nicht: Loader lädt nach.</li>
<li>Nützlich falls grosse Codesegmente selten benötigt</li>
<li>User-Space Implementation möglich</li>
</ul>

<h2>Dynamisches Binden</h2>

<ul>
<li>Stub ist Stv für aufgerufene Routine</li>
<li>Prüft ob Routine geladen, lädt wenn nicht</li>
<li>Ersetzt sich selbst mit Routine</li>
<li>Bibliotheksupdates ohne Compilierung/Linkung</li>
<li>Dynamic Link Library / SHared Library</li>
<li>Teilen von Code-Segmenten</li>
<li>Erfodert OS, da Wissen über geladene Routinen dort</li>
</ul>

<h2>Logische und Physikalische Adressen</h2>

<ul>
<li>Physikalisch: 0 - n auf RAM</li>
<li>Logisch: 0 - m in Prozessbereich.</li>
<li>Logisch + Offset = Physikalisch</li>
<li>Limit-Register: Logische Adresse kleiner Limit (0-basiert)</li>
</ul>

<h2>Hauptspeicherverwaltung</h2>

<ul>
<li>Linked lists, zB Bitmaps (frei / belegt)</li>
<li>Allocation: Schnell, wenig verschnitt, ...</li>
<li>First Fit, Next fit (Fortsezten bei Ende letzter Suche), Best fit, Worst fit, Quick fit (Liste mit üblichen Löcheern)</li>
<li>Buddy System: Blöcke sind Potenzen von 2. Kein externern, sondern nur interner, Verschnitt. Sehr schnell.</li>
</ul>

<h2>Paging</h2>

<p>Nicht-zusammenhängender physikalischer Adressraum.</p>

<ul>
<li>Aufteilung physikalischer Speicher in Kacheln (Frames, 2*n Bytes)</li>
<li>Logischer Speicher in Pages (Gleiche Grösse)</li>
<li>Paging-Table mapped Pages auf Frames</li>
<li>Internet Verschnitt</li>
<li>Paging Table per Prozess</li>
<li>Adresse: Page number p, Page offset 0, Frame nr k</li>
</ul>

<h3>Multi-level Paging</h3>

<p>Problem: Seitentabelle wird gross. (2<strong>32 Bytes, 4 KB Page, 2</strong>20 Pages, 4 Bytes per table entry, 4MB Page table)</p>

<p>Lösung: Multi-level Paging. Logische Adresse = (p1, p2, 0), Lookup in mehreren Nested Tables</p>

<h4>Two-Level paging</h4>

<p>Klar</p>

<h4>Page table with hashes</h4>

<h4>Inverted page table</h4>

<p>Nur Einträge für reale (belegte) Frames, Aber: Aufwändigere Suche</p>

<h2>Segmentierung</h2>

<ul>
<li>Compiler erstellt Segmente für lokale Variabeln, Stack, Prozedur A, Prozedur B, ....</li>
<li>Jedes Segment mit eigenem Offset (+ Limit)</li>
<li>Kombinierbar mit Paging</li>
</ul>

<h1>08</h1>

<h2>Virtueller Speicher</h2>

<ul>
<li>Benötigte Teile eines Programms in Hauptspeicher, rest in Sekundärspeicher</li>
<li>Grosser Speicherbereich</li>
<li>Memory sharing</li>
<li>Demand-Paging</li>
</ul>

<h2>Demand Paging</h2>

<ul>
<li>Pager lagert nur benötigte Pages ein (Weniger IO, weniger Speicherbedarf)</li>
<li>Memory Access: Laden der Page von Disk nach Memory if nicht geladen</li>
<li>Zugriff auf Page in Memory</li>
<li>Copy-on-write für bessere Performance</li>
</ul>

<h3>Paging Algorithmen</h3>

<ul>
<li>FIFO: Trivial: Schlechte Performance</li>
<li>LRU: Least recently used: Aufwändig</li>
<li>Referenzbit (1 Bit: Second chance)</li>
<li>Clock: 1 Ptr Auslagern, 1 Ptr zurücksetzen</li>
<li>Second chance+: 2 Bit, 1 bit second chance, 1 bit unverändert / verändert</li>
</ul>

<h2>Paging</h2>

<ul>
<li>Page buffering: Freihalten eines Pools an freien Pages: Schnellere Einlagerung</li>
<li>Schreiben Pages auf Speicher wenn Idle -> Eventuell schnellere Auslagerung später</li>
<li>Thrashing: Mehr mit Paging beschäftigt als mit Arbeit: Prozess mehr Kacheln zuordnen</li>
<li>Überwachung Page Faults: Anpassung Speicherzuordnung des Prozesses</li>
</ul>

<h1>09</h1>

<h2>Disk-Anbindung</h2>

<ul>
<li>Host-Anbindung: IDE, SATA, SCSI, ...</li>
<li>Netz-Anbindung: LAN, RCPs, iSCSI</li>
<li>SAN: Dediziertes Netz, spezielle Disk-Access Protokolle (Fibre-Channel, Infiniband, ...)</li>
</ul>

<h2>Disk Formatierung</h2>

<ul>
<li>Low-level / physikalisch: Unterteilen einer Disk in Sektoren. (Header - Data - Trailer)</li>
<li>Logisch: Partitionen, Filesystems</li>
</ul>

<h3>Partitionierung</h3>

<ul>
<li>MBR: In Sektor 0</li>
<li>Boot Block: Program das OS lädt</li>
<li>Superblock: Datenträgeraufbau, Blockgrösse, ... Meta Informationen</li>
<li>Freispeicherliste / Liste fehlender Blöcke</li>
</ul>

<h3>Fehlerhafte Blöcke</h3>

<ul>
<li>Sector Sparing: Liste schlechter Blöcke, transparente Umleitung auf Reserveblock -> Verschlechtertes Disk-SCheduling</li>
<li>Sector Slipping: Verschieben von Sektoren um eine Spur</li>
</ul>

<h3>RAID</h3>

<ul>
<li>Schützt vor Hardwarefehlern, aber nicht Software (z.B fehlerhaftem Disk-Driver)</li>
<li>Dort muss zB das FS (siehe ZFS) Abhilfe schaffen (Prüfsummen + Korrektur falls zB gespiegelt)</li>
</ul>

<h4>RAID 0</h4>

<ul>
<li>Striping / Interleaving: Jede Disk hat Streifen der virtuellen Disk</li>
<li>Block-level Striping: 1 Datei auf N Disks</li>
</ul>

<h4>RAID 1</h4>

<ul>
<li>Mirroring</li>
</ul>

<h4>RAID 2</h4>

<ul>
<li>Bit interleaving (7-bit hamming code)</li>
</ul>

<h4>RAID 3</h4>

<ul>
<li>RAID 2 mit nur Parität (even or odd)</li>
</ul>

<h4>RAID 4</h4>

<ul>
<li>Paritätsblöcke</li>
</ul>

<h4>RAID 5</h4>

<ul>
<li>Verteilung &amp; Parität</li>
</ul>

<h4>RAID 6</h4>

<ul>
<li>Wie 5, aber mit mehr Redundanz</li>
</ul>

<h2>Dateisysteme</h2>

<ul>
<li>Lesen &amp; Schreiben</li>
<li>Attribute per Datei (Namen, Ort, ...)</li>
<li>Multi-user</li>
</ul>

<h2>Trivia</h2>

<ul>
<li>mmap: Einbindung einer Datei als virtueller Speicher: Schreiben nach schliessen der Datei</li>
</ul>

<h1>10</h1>

<h2>Dateisysteme</h2>

<h3>Logisches Dateisystem</h3>

<ul>
<li>Datei und Verzeichnisoperationen</li>
<li>Verwalten von Dateien / Strukturen</li>
<li>Schutzmechanismen</li>
</ul>

<h3>Organisationsmodul</h3>

<ul>
<li>Übersetzung logische in physikalische Adresse</li>
<li>Freispeicherverwaltung</li>
<li>Festplattenmgmt</li>
</ul>

<h3>Basisdateisystem</h3>

<ul>
<li>Kommandübergabe an I/O (Lese Disk 1, Zylinder 2, Spur 3, Sektor 4)</li>
<li>Scheduling</li>
<li>Caching</li>
</ul>

<h3>I/O Steuerung</h3>

<ul>
<li>Interface zu Gerätetreiber, Interrupts</li>
</ul>

<h2>inode</h2>

<ul>
<li>mode, link count, owner uid, gid, ...</li>
<li>direct blocks (10) -> pointing to data blocks</li>
<li>single indirect -> points to list of direct blocks</li>
<li>double indirect -> points to list of single indirects</li>
<li>triple indirect -> ....</li>
</ul>

<h2>File table</h2>

<ul>
<li>Systemweite Tabelle mit offenen Files</li>
<li>Tracking welche Prozesse welche Files</li>
<li>Datei öffnen: FCB wird in file table kopiert, ausgabe Deskriptor für Zugriff</li>
</ul>

<h2>Verzeichnisse</h2>

<ul>
<li>Liste mit Zeigern auf Dateiblöcke: Einfach, non-performant (Varianten: Bäume, sortierte Listen, ...)</li>
<li>Hash-Tabelle: Berechnung Hash Wert aus Dateiname, dann Suche in Hash Bucket</li>
</ul>

<h2>Allokation</h2>

<h3>Zusammenhängend</h3>

<ul>
<li>Einfache Implementierung</li>
<li>Dateien können nicht wachsen</li>
<li>Wahlfreier Zugriff</li>
<li>Externer Verschnitt</li>
<li>Allokation zB best, worst, ... fit</li>
</ul>

<h3>Verkettete</h3>

<ul>
<li>Datei ist Liste von Blöcken</li>
<li>Beliebige Anordnung</li>
<li>Sequenzieller Zugriff, aber nicht wahlfrei</li>
<li>Keine Verschwendung</li>
<li>Bei beschädigtem Block ganze Datei weg</li>
<li>Beispiel: FAT. Unbenutzte mit 0 markiert</li>
</ul>

<h3>Indizierte</h3>

<ul>
<li>Alle Zeiger in Indexblock</li>
<li>Wahlfreier Zugriff</li>
<li>Kein Verschnitt</li>
<li>Overhead durch Index Block</li>
</ul>

<h2>Freispeicherverwaltung</h2>

<ul>
<li>Bitvektoren / Bitmaps (0: Block Frei, 1: Block Belegt)</li>
<li>Linked List (effizienter da nur freie gespeichert)</li>
<li>Gruppieren: Erste n freie Blöcke in einem Block grupieren (Schnell für grosse Mengen freier Platz finden)</li>
<li>Zählen: Linked list, Zeiger und Anzahl folgender freier Blöcke (inklusive Block selber)</li>
<li>Space Maps: (ZFS): Partition in Metaslabs, jeder 1 Space Map = Log aller Aktivitäten (Allokation + Freigabe), Aufbau als z.B Baum</li>
</ul>
